{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca34862",
   "metadata": {},
   "source": [
    "Unsupervised Machine Learning, we analyse and cluster unlabeled data. Unline Supervised, we are not given the true labels. All we are doing is clustering the data and finding patterns within the data!\n",
    "\n",
    "These algo are used to find hidden patterns in the data or data groupings!\n",
    "\n",
    "Ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397ba9d",
   "metadata": {},
   "source": [
    "https://www.ibm.com/topics/unsupervised-learning\n",
    "\n",
    "## 3 Main Tasks\n",
    "Read the names and think what they may do!\n",
    "\n",
    "### Clustering\n",
    "In clustering we group the data based on similarities and differences.\n",
    "\n",
    "We process raw, unclassified data into groups based on patterns\n",
    "\n",
    "#### 4 Types of Clustering Algorithms\n",
    "\n",
    "##### Exclusive\n",
    "It says that a given data point can only belong to 1 cluster! AKA \"hard\" clustering.\n",
    "\n",
    "example: K-means clustering: In this the data is assigned into one of K groups based on it's distance from the centroid of each group. A larger K value will be indicative of smaller groupings with more granularity whereas a smaller K value will have larger groupings and less granularity. K-means clustering is commonly used in market segmentation, document clustering, image segmentation, and image compression.\n",
    "\n",
    "##### Overlapping\n",
    "A data point can belong to more than 1 cluster, with different degree of membership. AKA \"soft\" or fuzzy k-means clustering\n",
    "\n",
    "##### Hierarchial\n",
    "Hierarchical clustering, also known as hierarchical cluster analysis (HCA), is an unsupervised clustering algorithm that can be categorized in two ways; they can be agglomerative or divisive. Agglomerative clustering is considered a “bottoms-up approach.” Its data points are isolated as separate groupings initially, and then they are merged together iteratively on the basis of similarity until one cluster has been achieved. Four different methods are commonly used to measure similarity:\n",
    "\n",
    "    Ward’s linkage: This method states that the distance between two clusters is defined by the increase in the sum of squared after the clusters are merged.\n",
    "    Average linkage: This method is defined by the mean distance between two points in each cluster\n",
    "    Complete (or maximum) linkage: This method is defined by the maximum distance between two points in each cluster\n",
    "    Single (or minimum) linkage: This method is defined by the minimum distance between two points in each cluster\n",
    "\n",
    "Euclidean distance is the most common metric used to calculate these distances; however, other metrics, such as Manhattan distance, are also cited in clustering literature.\n",
    "\n",
    "Divisive clustering can be defined as the opposite of agglomerative clustering; instead it takes a “top-down” approach. In this case, a single data cluster is divided based on the differences between data points. Divisive clustering is not commonly used, but it is still worth noting in the context of hierarchical clustering. These clustering processes are usually visualized using a dendrogram, a tree-like diagram that documents the merging or splitting of data points at each iteration.\n",
    "\n",
    "BEST WAY TO UNDERSTAND THIS, IS TO THINK ABOUT HOW THE BELOW GRAPH CAN BE FORMED BY THINKING OF IRIS DATASET\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "##### Probabilistic\n",
    "In probabilistic clustering, data points are clustered based on the likelihood that they belong to a particular distribution. \n",
    "\n",
    "example: Gaussian Mixture Models: classified as mixture models, which means that they are made up of an unspecified number of probability distribution functions. GMMs are primarily leveraged to determine which Gaussian, or normal, probability distribution a given data point belongs to. If the mean or variance are known, then we can determine which distribution a given data point belongs to. However, in GMMs, these variables are not known, so we assume that a latent, or hidden, variable exists to cluster data points appropriately. While it is not required to use the Expectation-Maximization (EM) algorithm, it is a commonly used to estimate the assignment probabilities for a given data point to a particular data cluster.   \n",
    "\n",
    "\n",
    "### Association\n",
    "An association rule is a rule-based method for finding relationships between variables in a given dataset. Enables businesses to develop better cross-selling strategies and recommendation engines. There are a few different algorithms used to generate association rules, such as Apriori, Eclat, and FP-Growth, the Apriori algorithm is most widely used.\n",
    "\n",
    "##### Apriori algorithms\n",
    "Apriori algorithms have been popularized through market basket analyses, leading to different recommendation engines for music platforms and online retailers. Apriori algorithms use a hash tree to count itemsets, navigating through the dataset in a breadth-first manner.\n",
    "\n",
    "\n",
    "### Dimensionality Reduction\n",
    "It is commonly used in the preprocessing data stage. Common methods to do this are:\n",
    "\n",
    "##### Principal component analysis (PCA)\n",
    "\n",
    "##### Singular Value Decomposition (SVD)\n",
    "\n",
    "##### Autoencoders\n",
    "\n",
    "## Challenges of UL\n",
    "essentially the machine is trying to find patterns in data, so it requires a huge amount of data to find the CORRECT patterns, also these \"correct\" (everything is correct for machines, however it may not be what we want, think wrt to image detection) observations need to human validated.\n",
    "\n",
    "    Computational complexity due to a high volume of training data\n",
    "    Longer training times\n",
    "    Higher risk of inaccurate results\n",
    "    Human intervention to validate output variables\n",
    "    Lack of transparency into the basis on which data was clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fc587",
   "metadata": {},
   "source": [
    "### VIDEO\n",
    "supervised uses labelled data, however unsupervised does not\n",
    "\n",
    "there is also semi-supervised, in which the some of the correct labels are given\n",
    "\n",
    "In SL, since correct labels are given, the model can measure it's accuracy and learn over time\n",
    "\n",
    "The right model for your data, depends on the type of data that you have and what you want to do with it\n",
    "\n",
    "SL --> PREDICT\n",
    "\n",
    "UL --> GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba83fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
